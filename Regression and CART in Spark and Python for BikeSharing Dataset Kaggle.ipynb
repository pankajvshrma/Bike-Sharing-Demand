{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/osboxes/new_hour.csv\"\n",
    "raw_data = sc.textFile(path)\n",
    "num_data = raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n"
     ]
    }
   ],
   "source": [
    "records = raw_data.map(lambda x: x.split(\",\"))\n",
    "first = records.first()\n",
    "print first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feasture column: {u'1': 0, u'3': 1, u'2': 2, u'4': 3}\n"
     ]
    }
   ],
   "source": [
    "print \"Mapping of first categorical feasture column: %s\" % get_mapping(records, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mappings = [get_mapping(records, i) for i in range(2,10)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(records.first()[11:15])\n",
    "total_len = num_len + cat_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 57\n",
      "Feature vector length for numerical features: 4\n",
      "Total feature vector length: 61\n"
     ]
    }
   ],
   "source": [
    "print \"Feature vector length for categorical features: %d\" % cat_len\n",
    "print \"Feature vector length for numerical features: %d\" % num_len\n",
    "print \"Total feature vector length: %d\" % total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec, num_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_label(record):\n",
    "    return float(record[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: [u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n",
      "Label: 16.0\n",
      "Linear Model feature vector:\n",
      "[1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0]\n",
      "Linear Model feature vector length: 61\n"
     ]
    }
   ],
   "source": [
    "first_point = data.first()\n",
    "print \"Raw data: \" + str(first[2:])\n",
    "print \"Label: \" + str(first_point.label)\n",
    "print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
    "print \"Linear Model feature vector length: \" + str(len(first_point.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[2:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree feature vector: [1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]\n",
      "Decision Tree feature vector length: 12\n"
     ]
    }
   ],
   "source": [
    "data_dt = records.map(lambda r: LabeledPoint(extract_label(r),extract_features_dt(r)))\n",
    "first_point_dt = data_dt.first()\n",
    "print \"Decision Tree feature vector: \" + str(first_point_dt.features)\n",
    "print \"Decision Tree feature vector length: \" + str(len(first_point_dt.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module pyspark.mllib.regression:\n",
      "\n",
      "train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0, initialWeights=None, regParam=0.0, regType=None, intercept=False, validateData=True, convergenceTol=0.001) method of __builtin__.type instance\n",
      "    Train a linear regression model using Stochastic Gradient\n",
      "    Descent (SGD).\n",
      "    This solves the least squares regression formulation\n",
      "    \n",
      "        f(weights) = 1/(2n) ||A weights - y||^2,\n",
      "    \n",
      "    which is the mean squared error.\n",
      "    Here the data matrix has n rows, and the input RDD holds the\n",
      "    set of rows of A, each with its corresponding right hand side\n",
      "    label y. See also the documentation for the precise formulation.\n",
      "    \n",
      "    :param data:              The training data, an RDD of\n",
      "                              LabeledPoint.\n",
      "    :param iterations:        The number of iterations\n",
      "                              (default: 100).\n",
      "    :param step:              The step parameter used in SGD\n",
      "                              (default: 1.0).\n",
      "    :param miniBatchFraction: Fraction of data to be used for each\n",
      "                              SGD iteration (default: 1.0).\n",
      "    :param initialWeights:    The initial weights (default: None).\n",
      "    :param regParam:          The regularizer parameter\n",
      "                              (default: 0.0).\n",
      "    :param regType:           The type of regularizer used for\n",
      "                              training our model.\n",
      "    \n",
      "                              :Allowed values:\n",
      "                                 - \"l1\" for using L1 regularization (lasso),\n",
      "                                 - \"l2\" for using L2 regularization (ridge),\n",
      "                                 - None for no regularization\n",
      "    \n",
      "                                 (default: None)\n",
      "    \n",
      "    :param intercept:         Boolean parameter which indicates the\n",
      "                              use or not of the augmented representation\n",
      "                              for training data (i.e. whether bias\n",
      "                              features are activated or not,\n",
      "                              default: False).\n",
      "    :param validateData:      Boolean parameter which indicates if\n",
      "                              the algorithm should validate data\n",
      "                              before training. (default: True)\n",
      "    :param convergenceTol:    A condition which decides iteration termination.\n",
      "                              (default: 0.001)\n",
      "    \n",
      "    .. versionadded:: 0.9.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "help(LinearRegressionWithSGD.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method trainRegressor in module pyspark.mllib.tree:\n",
      "\n",
      "trainRegressor(cls, data, categoricalFeaturesInfo, impurity='variance', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0) method of __builtin__.type instance\n",
      "    Train a DecisionTreeModel for regression.\n",
      "    \n",
      "    :param data: Training data: RDD of LabeledPoint.\n",
      "                 Labels are real numbers.\n",
      "    :param categoricalFeaturesInfo: Map from categorical feature\n",
      "             index to number of categories.\n",
      "             Any feature not in this map is treated as continuous.\n",
      "    :param impurity: Supported values: \"variance\"\n",
      "    :param maxDepth: Max depth of tree.\n",
      "             E.g., depth 0 means 1 leaf node.\n",
      "             Depth 1 means 1 internal node + 2 leaf nodes.\n",
      "    :param maxBins: Number of bins used for finding splits at each\n",
      "             node.\n",
      "    :param minInstancesPerNode: Min number of instances required at\n",
      "             child nodes to create the parent split\n",
      "    :param minInfoGain: Min info gain required to create a split\n",
      "    :return: DecisionTreeModel\n",
      "    \n",
      "    Example usage:\n",
      "    \n",
      "    >>> from pyspark.mllib.regression import LabeledPoint\n",
      "    >>> from pyspark.mllib.tree import DecisionTree\n",
      "    >>> from pyspark.mllib.linalg import SparseVector\n",
      "    >>>\n",
      "    >>> sparse_data = [\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\n",
      "    ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\n",
      "    ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\n",
      "    ... ]\n",
      "    >>>\n",
      "    >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\n",
      "    >>> model.predict(SparseVector(2, {1: 1.0}))\n",
      "    1.0\n",
      "    >>> model.predict(SparseVector(2, {1: 0.0}))\n",
      "    0.0\n",
      "    >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\n",
      "    >>> model.predict(rdd).collect()\n",
      "    [1.0, 0.0]\n",
      "    \n",
      "    .. versionadded:: 1.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTree.trainRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10,step=0.1, intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model predictions: [(16.0, 117.89250386724844), (40.0, 116.22496123192109), (32.0, 116.02369145779232), (13.0, 115.67088016754431), (1.0, 115.56315650834316)]\n"
     ]
    }
   ],
   "source": [
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree predictions: [(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945), (13.0, 14.284023668639053), (1.0, 14.284023668639053)]\n",
      "Decision Tree depth: 5\n",
      "Decision Tree number of nodes: 63\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTree.trainRegressor(data_dt,{})\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "actual = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))\n",
    "print \"Decision Tree depth: \" + str(dt_model.depth())\n",
    "print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error(actual, pred):\n",
    "    return (pred - actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model - Mean Squared Error: 30679.4539\n",
      "Linear Model - Mean Absolute Error: 130.6429\n",
      "Linear Model - Root Mean Squared Log Error: 1.4653\n"
     ]
    }
   ],
   "source": [
    "print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
    "print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
    "print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Mean Squared Error: 11560.7978\n",
      "Decision Tree - Mean Absolute Error: 71.0969\n",
      "Decision Tree - Root Mean Squared Log Error: 0.6259\n"
     ]
    }
   ],
   "source": [
    "mse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt\n",
    "print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt\n",
    "print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
